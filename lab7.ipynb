{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Sentiment Analysis on Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Total points 130)\n",
    "\n",
    "In this lab, you will gain hands on experiences on processing the raw text data for sentiment analysis.   This lab is divided into two subparts as follows: \n",
    "\n",
    "1. Treat sentiment analysis as a process of word matching, which you are given lexicons for both sets of positive and negative words.\n",
    "\n",
    "2. Think of sentiment analysis as a binary classification problem that a binary classifier need to be learned from a set of training samples and generalize to any other unseen samples.\n",
    "\n",
    "The purpose of this lab is to guide you steps by steps and after this lab you will at least have a general sense of academic research in the field of text mining, NLP, etc.  \n",
    "\n",
    "Beside this ipython notebook, you are also given a list of text files as follows:\n",
    "\n",
    "1.  pros_new.txt: contains 500 “positive” text data for you to analyze.\n",
    "2.  cons_new.txt: contains 500 “negative” text data for you to analyze.\n",
    "3.  positive-words.txt: positive word lexicon.\n",
    "4.  negative-words.txt: negative word lexicon.\n",
    "5. stopwords.txt:  a list of stop words.\n",
    "\n",
    "You will use all these files through the lab, and it will be explained more in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What to hand in: \n",
    "You will need to pack following things into a file.\n",
    "\n",
    "\n",
    "   * The completed Notebook file (ipynb) - Remember to answer all the questions in the notebook!\n",
    "   * All the figures plotted in this lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please go through the instructions in the notebook thoroughly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sentiment Analysis as Word Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure all the following python packages are successfully installed, you need to use them for this lab.\n",
    "\n",
    "Use  one of the following commands if needed:\n",
    "\n",
    "pip install nltk  \n",
    "\n",
    "\n",
    "pip install nltk --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys;\n",
    "import numpy as np;\n",
    "import scipy.sparse;\n",
    "import nltk;\n",
    "from nltk.stem.snowball import SnowballStemmer;\n",
    "import random;\n",
    "import math;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will process all 1000 text data from pros_new.txt and cons_new.txt (both positive and negative) to match the word in positive or negative lexicons. An example of the type of data you will see is:\n",
    "\n",
    "“Holding the camera steady for 5 seconds after taking picture, Not XP compatible Too little space to answer”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in the codes in the blanks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first work with an example line, and then combine all the code to create your sentiment analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:  Token the string and remove punctuation using regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line = \"Holding the camera steady for 5 seconds after taking picture, Not XP compatible Too little space to answer.\";\n",
    "\n",
    "# define the toker\n",
    "toker = nltk.tokenize.RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True);\n",
    "tokens = toker.tokenize(line);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Convert each word to a lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['holding', 'the', 'camera', 'steady', 'for', '5', 'seconds', 'after', 'taking', 'picture', 'not', 'xp', 'compatible', 'too', 'little', 'space', 'to', 'answer']\n"
     ]
    }
   ],
   "source": [
    "tokens_new = []\n",
    "for word in tokens:     \n",
    "    if word not in {' ', ',' , '.'}:\n",
    "    \n",
    "        #### 1. FILL IN SOME CODE LINE BELOW ####\n",
    "        # (5 points)\n",
    "        \n",
    "        # tokens_new should contain the list of words in lower case.\n",
    "        tokens_new.append(word.lower())\n",
    "                \n",
    "print tokens_new\n",
    "tokens = tokens_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load the stop words list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('stopwords.txt','r');\n",
    "stoplist = set([]);\n",
    "for line in f:\n",
    "    line = line.strip();\n",
    "    line = set([line]);\n",
    "    stoplist |= line;\n",
    "f.close;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Remove all stop words from the tokens list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 2. WRITE YOUR CODE HERE ####\n",
    "# (10 points)\n",
    "\n",
    "# remove the words in the 'tokens' list which are in stoplist\n",
    "# return a list called tokens\n",
    "new_list = []\n",
    "for word in tokens:\n",
    "    if word in stoplist:\n",
    "        continue\n",
    "    else:\n",
    "        new_list.append(word)\n",
    "        \n",
    "tokens = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:  Stem each word:\n",
    "\n",
    "Stemming is the process of converting the words of a sentence to its non-changing portions. In the example of amusing, amusement, and amused, the stem would be amus(e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hold', 'camera', 'steadi', '5', 'second', 'take', 'pictur', 'xp', 'compat', 'space', 'answer']\n"
     ]
    }
   ],
   "source": [
    "# define the stemmer \n",
    "stemmer = SnowballStemmer(\"english\");\n",
    "\n",
    "# stem all the words in the tokens list. This is a standard way to step words.\n",
    "tokens = [ str(stemmer.stem(word)) for word in tokens];\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5(a):  Load the postive lexicons.\n",
    "\n",
    "This should be similar to step 2 above. Also stem each word, similar to step 4 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> The list of first 15 postive words after stemming: \n",
      "['unencumb', 'pardon', 'saver', 'desir', 'encourag', 'sleek', 'thought', 'cooper', 'fair', 'faster', 'work', 'undisput', 'sturdi', 'envious', 'homag']\n"
     ]
    }
   ],
   "source": [
    "#### 3. COMPLETE THE CODE BELOW ####\n",
    "# (15 points)\n",
    "# read the pos lexicon file\n",
    "\n",
    "file_pos = open(\"positive-words.txt\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "poslist = set([]);\n",
    "for line in file_pos:\n",
    "    # 1. Use the strip() function to remove the end-of-line tags\n",
    "    # 2. Stem the line (similar to the previous stemming code. Remember, each line is a word in these text files.)\n",
    "    # 3. Add the stemmed line to the postive list.\n",
    "    line = line.strip()\n",
    "    line = set([line])\n",
    "    poslist |= line\n",
    "\n",
    "poslist = [str(stemmer.stem(word)) for word in poslist]\n",
    "file_pos.close();\n",
    "\n",
    "print \"==> The list of first 15 postive words after stemming: \"\n",
    "print list(poslist)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5(b): Now do the same for the negative lexicon file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> The list of first 15 negative words after stemming: \n",
      "['limit', 'subtract', 'belliger', 'suicid', 'cuss', 'inadequaci', 'dissolut', 'refut', 'threaten', 'foul', 'obstruct', 'protest', 'slog', 'lurk', 'thirst']\n"
     ]
    }
   ],
   "source": [
    "#### 4. WRITE YOUR CODE HERE ####\n",
    "# (5 points)\n",
    "\n",
    "file_neg = open(\"negative-words.txt\")\n",
    "neglist = set([]);\n",
    "\n",
    "for line in file_neg:\n",
    "    line = line.strip()\n",
    "    line = set([line])\n",
    "    neglist |= line\n",
    "    \n",
    "neglist = [str(stemmer.stem(word)) for word in neglist]\n",
    "file_neg.close()\n",
    "print \"==> The list of first 15 negative words after stemming: \"\n",
    "print list(neglist)[:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This is not the best way to write code. If one is using the same set of steps multiple times, one should define a function that does the required processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Match your string tokens with both lexicons: If the number of positive words larger than the negative ones, you will report this as a positive sentence, vise versa.  \n",
    "\n",
    "NOTES:  If the number of postive words is the same as the negative ones, you will report it as neutral/not sure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos words:  2\n",
      "Neg words:  0\n"
     ]
    }
   ],
   "source": [
    "# Create a list of positive words and\n",
    "# negative words in the list 'tokens'.\n",
    "# Print the number of positive and negative words \n",
    "\n",
    "\n",
    "#### 5. FILL IN BELOW ####\n",
    "# (5 points, including answer)\n",
    "\n",
    "pos_word_list = []\n",
    "neg_word_list = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token in poslist:\n",
    "        pos_word_list.append(token)\n",
    "    elif token in neglist:\n",
    "        neg_word_list.append(token)\n",
    "\n",
    "print \"Pos words: \", len(pos_word_list)\n",
    "print \"Neg words: \", len(neg_word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "Based on the number of postive and negative words, the line above is \n",
    "\n",
    "(a) Positive (b) Negative (c) Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer:\n",
    "Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Combine it all!\n",
    "\n",
    "Now that we have all the tools to process the data for sentiment analysis, we will be analyzing the two files \"pros_new.txt\" and “cons_new.txt”  line by line (each line contains one sentence). \n",
    "\n",
    "To do so, we define the function sentiment_analyzer which takes in filename, postive list and negative list. The function returns the number of positive, negative and neutral sentences, and the number of sentences in the document. \n",
    "\n",
    "Report the accuracy you obtained for each file separately as well as how many percentage of the positive sentence you categorized as negative (the miss detection rate) and how many percentage of the negative sentence you categorized as positive (the false alarm rate).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use the pros text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### 6. WRITE YOUR CODE HERE ###############\n",
    "# (25 points)\n",
    "\n",
    "def sentiment_analyzer(filename, poslist, neglist):\n",
    "    \n",
    "    # 1. Load the tokenizer. \n",
    "    # 2. Set postive, negative and neutral counts to zero. \n",
    "    # 3. Read the file and keep track of the positive and negative words lists. \n",
    "    # 4. Then check if the document is more positive, or more negative. If equal, report 'Not Sure'. \n",
    "    # 5. Then print the accuracy and missed detection rate.\n",
    "\n",
    "    f = open('pros_new.txt','r');\n",
    "    num_lines = 0\n",
    "    pos_count_total = 0\n",
    "    neg_count_total = 0\n",
    "    neutral_count_total = 0\n",
    "    \n",
    "    for line in f:\n",
    "        toker = nltk.tokenize.RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True);\n",
    "        tokens = toker.tokenize(line);\n",
    "\n",
    "        tokens_new = []\n",
    "        for word in tokens:     \n",
    "            if word not in {' ', ',' , '.'}:\n",
    "                tokens_new.append(word.lower())\n",
    "\n",
    "        new_list = []\n",
    "        for word in tokens:\n",
    "            if word in stoplist:\n",
    "                continue\n",
    "            else:\n",
    "                new_list.append(word)\n",
    "        tokens = new_list\n",
    "        tokens = [str(stemmer.stem(word)) for word in tokens]\n",
    "\n",
    "        pos_word_list = []\n",
    "        neg_word_list = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in poslist:\n",
    "                pos_word_list.append(token)\n",
    "            elif token in neglist:\n",
    "                neg_word_list.append(token)\n",
    "        \n",
    "        if len(pos_word_list) > len(neg_word_list):\n",
    "            pos_count_total += 1\n",
    "        elif len(neg_word_list) > len(pos_word_list):\n",
    "            neg_count_total += 1\n",
    "        else:\n",
    "            neutral_count_total += 1\n",
    "        \n",
    "        num_lines += 1\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return pos_count_total, neg_count_total, neutral_count_total, num_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above function to process the pros_new.txt document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_correct:  345\n",
      "p_wrong:  33\n",
      "p_notsure :  122\n",
      "p_numlines:  500\n",
      "Accuracy:  0.69\n",
      "Miss detection rate:  0.066\n"
     ]
    }
   ],
   "source": [
    "############### 7. WRITE YOUR CODE HERE ###############\n",
    "# (5 points, including answers to questions below)\n",
    "\n",
    "p_correct, p_wrong, p_notsure, p_num_lines = sentiment_analyzer(\"pros_new.txt\", poslist, neglist)\n",
    "\n",
    "print \"p_correct: \", p_correct\n",
    "print \"p_wrong: \", p_wrong\n",
    "print \"p_notsure : \", p_notsure\n",
    "print \"p_numlines: \", p_num_lines\n",
    "print \"Accuracy: \", float(p_correct)/float(p_num_lines)\n",
    "print \"Miss detection rate: \", float(p_wrong)/float(p_num_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [WRITE YOUR ANSWERS BELOW. EXPLAIN IF NECESSARY.] ####\n",
    "\n",
    "Number of sentence you think is postive: 345\n",
    "\n",
    "Number of sentence you think is negative: 33\n",
    "\n",
    "Number of sentence you think is Neutral: 122\n",
    "\n",
    "Accuarcy: 0.69\n",
    "\n",
    "Miss detection rate: 0.066\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeat for the cons text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_correct:  33\n",
      "n_wrong:  345\n",
      "n_notsure :  122\n",
      "n_numlines:  500\n"
     ]
    }
   ],
   "source": [
    "############### 8. WRITE YOUR CODE HERE ###############\n",
    "# (5 points, including answers to questions below)\n",
    "\n",
    "n_wrong, n_correct, n_notsure, n_num_lines = sentiment_analyzer(\"cons_new.txt\", poslist, neglist)\n",
    "\n",
    "print \"n_correct: \", n_correct\n",
    "print \"n_wrong: \", n_wrong\n",
    "print \"n_notsure : \", n_notsure\n",
    "print \"n_numlines: \", n_num_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [WRITE YOUR ANSWERS BELOW. EXPLAIN IF NECESSARY.] ####\n",
    "\n",
    "Number of sentence you think is postive: 345\n",
    "\n",
    "Number of sentence you think is negative: 33\n",
    "\n",
    "Number of sentence you think is Neutral: 122\n",
    "\n",
    "Accuarcy: 0.066\n",
    "\n",
    "False Alarm rate: 0.69\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Report the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy:  0.378\n"
     ]
    }
   ],
   "source": [
    "#### 9. YOUR CODE HERE TO OUTPUT THE OVERALL ACCURACY ####\n",
    "# (5 points)\n",
    "\n",
    "# accuracy = number of total correct predictions / number of total sentences\n",
    "\n",
    "o_accuracy = (float(p_correct)+float(n_correct))/(float(n_num_lines) * 2)\n",
    "\n",
    "print \"Overall accuracy: \", o_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Sentiment Analysis as Binary Classifiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab, we will be using the Nearest Neighbor (NN) classifier to treat the sentiment analysis as a binary classification problem.  \n",
    "\n",
    "Each sentence is vectorized for you in some vector space.  And you need to randomly select 80% of positive and negative samples and use it as a training set, and the rest of data as testing set.  For each test sample, calculate the Euclidean distance to all the samples in the training set, and use the label associated with the data with least distance as your predicted label. \n",
    "\n",
    "Compare your predicted label with ground truth and report the accuracy.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first construct a dictionary of words, associating a number (corresponding to its entry in the dictionary) with the word.\n",
    "\n",
    "We will then convert each sentence to a 1,610 dimensional vectorized representation. We will encode this using a presence-absence matrix. Every sentence will have an entry corresponding to every word indicating whether the word is present in the sentence or not. This is one way of converting text to vectors. \n",
    "\n",
    "At the end, we will obtain two variables:\n",
    "\n",
    "1. pos_vec:  a 500 x 1610 numpy matrix, each row indicate the vector representation for the corresponding positive sentence in the pros_new.txt file.  \n",
    "\n",
    "2. neg_vec:  same as the pos_vec except that each row is generated from the cons_new.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of unique words: 1610\n"
     ]
    }
   ],
   "source": [
    "# Construct a full word list using dictionary. This code is provided to you.\n",
    "\n",
    "wordlist = {};\n",
    "counter = 0;\n",
    "f = open('pros_new.txt','r');\n",
    "for line in f: \n",
    "    line = line.strip();\n",
    "    tokens = toker.tokenize(line);\n",
    "    for word in tokens:\n",
    "        if not(word in wordlist) and word.isalpha():\n",
    "            wordlist[word] = counter;\n",
    "            counter += 1;\n",
    "f.close;\n",
    "f = open('cons_new.txt','r');\n",
    "for line in f: \n",
    "    line = line.strip();\n",
    "    tokens = toker.tokenize(line);\n",
    "    for word in tokens:\n",
    "        if not(word in wordlist) and word.isalpha():\n",
    "            wordlist[word] = counter;\n",
    "            counter += 1;\n",
    "f.close;\n",
    "\n",
    "print \"The total number of unique words: %d\" %(len(wordlist.keys()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define a function to obtain the binary vectors for any file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 10. YOUR CODE HERE ####\n",
    "# (15 points)\n",
    "\n",
    "def get_binary_vec(filename):\n",
    "    # construct binary feature vectors; \n",
    "    vec = np.zeros((500,len(wordlist.keys())));\n",
    "    f = open(filename,'r');\n",
    "    \n",
    "     \n",
    "    # 1. Load the tokenizer\n",
    "    toker = nltk.tokenize.RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "        \n",
    "    iteration = 0;\n",
    "    for line in f: \n",
    "        \n",
    "        # 1. Strip line to remove trailing line breaks\n",
    "        # 2. Tokenize the line\n",
    "        line = line.strip()\n",
    "        tokens = toker.tokenize(line)\n",
    "        \n",
    "        for word in tokens:\n",
    "            # 3. Go through every word in the line. If the word contains no numbers, \n",
    "            #    update the corresponding entry in the vec array to be 1.\n",
    "            \n",
    "            # To assign '1' to the correct indices - the first index corresponds\n",
    "            # to the sentence number and the second one corresponds to the \n",
    "            # column number as defined by the word_list dictionary\n",
    "            if word in wordlist.keys():\n",
    "                vec[iteration][wordlist[word]] = 1\n",
    "            \n",
    "            \n",
    "        iteration += 1;\n",
    "    f.close()\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run for both files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_vec = get_binary_vec('pros_new.txt')\n",
    "neg_vec = get_binary_vec('cons_new.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fast implementation of Euclidean distance computation, which will be a lot faster than if you are looping through all training and testing samples.  We will use this as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FAST_L2_distance(A,B):\n",
    "    # L2_distance conputes pairwise squared Euclidean distance matrix. \n",
    "    # Inputs: \n",
    "    #     A -- (d x m) matrix , m samples, d dimension\n",
    "    #     B -- (d x n) matrix , n samples, d dimension\n",
    "    #\n",
    "    # Outputs: \n",
    "    #     D -- (m x n) squared Euclidean distance matrix, (i,j) entry indicates the squared distance of ith sample in A and \n",
    "    #          jth sample in B.   \n",
    "    \n",
    "    d = A.shape[0];\n",
    "    # error checking:\n",
    "    if (B.shape[0] != d):\n",
    "        raise ValueError (\"Dimension mismatched\");\n",
    "    # A_norm[i] = ||A[:,i]||^2\n",
    "    A_norm = np.sum(A**2,axis=0);\n",
    "    A_norm = np.reshape(A_norm,(1,A_norm.shape[0]));\n",
    "    # print A_norm.shape;\n",
    "    B_norm = np.sum(B**2,axis=0);\n",
    "    B_norm = np.reshape(B_norm,(1,B_norm.shape[0]));\n",
    "    # print B_norm.shape;\n",
    "    cross = -2*np.dot(A.T,B);\n",
    "    # print cross.shape\n",
    "    # print A_norm.T.shape\n",
    "    # print np.tile(A_norm.T,(1,cross.shape[1]))\n",
    "    # print np.tile(B_norm,(cross.shape[0],1)).shape\n",
    "    D = cross + np.tile(A_norm.T,(1,cross.shape[1])) + np.tile(B_norm,(cross.shape[0],1));\n",
    "    # print D.shape\n",
    "    # print D\n",
    "    return D;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed for the random generator.  Please use \"2017\" as the seed for your final sumbission.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set a seed for the random generator \n",
    "random.seed(\"2017\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random select 80% of positive and negative samples and use them as your training data. To do so:\n",
    "   * First figure out how many samples will be in the training, and how many in the test.\n",
    "   * Then randomly sample indices in the range 0-500 to obtain the training and testing indices.\n",
    "   * Then select the appropriate subset of rows of pos_vec and neg_vec for training and testing. Append these to get            the final x_train and x_test.\n",
    "   * Finally assign '1' and '0' to the labels, based on whether they are from the positive file, or the negative file, and append appropriately to get y_train and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 800\n",
      "The number of testing data: 200\n"
     ]
    }
   ],
   "source": [
    "############### 11. WRITE YOUR CODE HERE ###############\n",
    "# (10 points)\n",
    "\n",
    "num_sample = 1000\n",
    "pos_idx_train = random.sample(range(0,500), 400)\n",
    "neg_idx_train = random.sample(range(0,500), 400)\n",
    "pos_idx_test = []\n",
    "neg_idx_test = []\n",
    "for i in range(500):\n",
    "    if i not in pos_idx_train:\n",
    "        pos_idx_test.append(i)\n",
    "    if i not in neg_idx_train:\n",
    "        neg_idx_test.append(i)\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for i in pos_idx_train:\n",
    "    x_train.append(pos_vec[i])\n",
    "    y_train.append(1)\n",
    "\n",
    "for j in neg_idx_train:\n",
    "    x_train.append(neg_vec[j])\n",
    "    y_train.append(0)\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "\n",
    "for i in pos_idx_test:\n",
    "    x_test.append(pos_vec[i])\n",
    "    y_test.append(1)\n",
    "    \n",
    "for j in neg_idx_test:\n",
    "    x_test.append(neg_vec[i])\n",
    "    y_test.append(0)\n",
    "\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "print \"The number of training data: %d\" %(x_train.shape[0]);\n",
    "print \"The number of testing data: %d\" %(x_test.shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the distance between each testing sample to each training sample. Finally, \n",
    "* Sort to find the element with the least distance, for each line\n",
    "* Assign the corresponding label\n",
    "* Compare to ground truth to get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "The nearest neighbor classifier accuarcy: 85.000000%\n"
     ]
    }
   ],
   "source": [
    "############### 12. WRITE YOUR CODE HERE ###############\n",
    "# (10 points, including average accuracy)\n",
    "\n",
    "D = FAST_L2_distance(x_test.transpose(), x_train.transpose())\n",
    "\n",
    "# sort and find the index \n",
    "# hint: argsort\n",
    "pred_idx = [np.argsort(line) for line in D]\n",
    "\n",
    "# assign the label of the nearest neighbor as the prediction for y\n",
    "pred_y = []\n",
    "\n",
    "for i in pred_idx:\n",
    "    pred_y.append(y_train[i][0])\n",
    "\n",
    "# compare to the groundtruth\n",
    "NN_correct =  sum(pred_y == y_test);\n",
    "print NN_correct\n",
    "\n",
    "# compute and print the accuracy\n",
    "NN_acc = NN_acc = float(NN_correct)/float(y_test.shape[0])*100.0;\n",
    "print \"The nearest neighbor classifier accuarcy: %f%%\" %(NN_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the learning experiment for ten times (By changing the last 3 blocks of code).\n",
    "Report your average accuarcy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Accuarcy: .83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: ( 5+5+5 points )\n",
    "* Compare your accuarcy from two subparts of this lab, which one is better? \n",
    "* Why? \n",
    "* Can you think about other ways to improve accuarcy in both ways?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, NN using euclidean distance was significantly better since whatever word we were looking at did not directly need to be in our positive words list or negative words list to be classified. Therefore, there were a lot more words being correctly classified.\n",
    "\n",
    "One big way to improve the accuracy in both conditions is to expand our list or positive and negative words. Therefore, the chances of direct hits is larger and there are more classified points to look at in the nearest neighbor prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
