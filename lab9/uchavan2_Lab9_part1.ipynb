{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Introduction to TensorFlow and Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(125 points + extra 15 points)\n",
    "\n",
    "This code is adapted from Chapter 15 of the 'Hands On Machine Learning with Scikit-Learn & Tensorflow' by A. Geron, ([reference](http://proquest.safaribooksonline.com/book/programming/9781491962282/15dot-autoencoders/autoencoders_chapter_html)) and the corresponding github repository.\n",
    "\n",
    "Please have a look at the chapter referenced to get a background on autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Tensorflow, you can use Anaconda. Here are the steps to follow:\n",
    "* Download the relevant Anaconda platform from [here](https://www.anaconda.com/download/)\n",
    "* Install TensorFlow using Anaconda, by using the relevant commands from [here](https://www.tensorflow.org/install/)\n",
    "\n",
    "It took me 5 minutes to install Tensorflow using this method, on my own Linux machine, and about 30 minutes to do so on the EWS Linux machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that you should be able to run Tensorflow. As a quick check, try to run 'import tensorflow as tf' in a jupyter notebook to verify. You can then proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What to hand in: \n",
    "You will need to pack following things into a file. **Please zip all these into one zip file**, with the name netID_lab9\n",
    "\n",
    "\n",
    "   * The completed Notebook file (ipynb) - Remember to answer all the questions in the notebook!\n",
    "   * All the figures plotted in this lab (The reconstruction of the autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy.random as rnd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"autoencoders\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(\"images\", fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a couple utility functions to plot grayscale 28x28 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image, shape=[28, 28]):\n",
    "    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_multiple_images(images, n_rows, n_cols, pad=2):\n",
    "    images = images - images.min()  # make the minimum == 0, so the padding looks white\n",
    "    w,h = images.shape[1:]\n",
    "    image = np.zeros(((w+pad)*n_rows+pad, (h+pad)*n_cols+pad))\n",
    "    for y in range(n_rows):\n",
    "        for x in range(n_cols):\n",
    "            image[(y*(h+pad)+pad):(y*(h+pad)+pad+h),(x*(w+pad)+pad):(x*(w+pad)+pad+w)] = images[y*n_cols+x]\n",
    "    plt.imshow(image, cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this will be the first time many of you will be using tensorflow, this section gives most of the basic code. Your task will be to fill in some missing code sections, while you get acquainted with the format of neural networks in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an overview of neural networks, you can visit https://deeplearning4j.org/neuralnet-overview. There are many online resources available. \n",
    "\n",
    "Another resource is the book 'Hands On Machine Learning with Scikit-Learn & Tensorflow' by A. Geron, whose ebook you can find in the library resources section. It is strongly encouraged to go through the chapters related to TensorFlow in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0: Load TensorFlow and MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import the tensorflow module\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to train autoencoders. We will train and visualize the reconstructions of some autoencoders in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Building a simple autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, let's build an autoencoder with just one hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear any existing tensorflow graphs. Tensorflow operates by creating graphs and nodes, \n",
    "# and passing data as tensors through the graph, during execution. It loads the exact\n",
    "# values of nodes during execution.\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that will train one autoencoder and return the transformed training set (i.e., the output of the hidden layer) and the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### FILL IN THE CODE HERE ##################################\n",
    "\n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                      learning_rate = 0.01, l2_reg = 0.0005,\n",
    "                      activation=tf.nn.elu, seed=42):\n",
    "   \n",
    "    # Tensorflow works with computation graphs, loading the needed data and variable values\n",
    "    # when a session is opened, using the tf.Session() \n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "    \n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        # Tensorflow uses placeholders to indicate that data/variables will be passed here \n",
    "        # during the execution phase. \n",
    "        # To define placeholders, we have to pass the type of \n",
    "        # the variable, and its shape as inputs. If you specify None for any dimension of \n",
    "        # shape, it means \"any size\". This dimension will then be inferred from the data\n",
    "        \n",
    "        # Create a placeholder with tf.float32 and \n",
    "        # shape such that the second dimension is always n_inputs, while the first depends \n",
    "        # on data. \n",
    "\n",
    "        # (5 points)\n",
    "        X = tf.placeholder(tf.float32, shape=(None,n_inputs))\n",
    "        \n",
    "        # Code to define a dense layer of connections. \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        # Create the first 'hidden layer' will map X to n_neurons in the hidden layer.\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        \n",
    "        # Fill the blanks below to build the output layer by mapping hidden (layer) to \n",
    "        # n_inputs (since n_outputs = n_inputs).\n",
    "\n",
    "        # (5 points)\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        # Fill in the blanks below to compute the reconstruction loss. \n",
    "        # Use the square() and reduce_mean() functions of TensorFlow\n",
    "        # to compute the mean square error between outputs and X. \n",
    "        \n",
    "        # (10 points)\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "        \n",
    "        # Define regularization loss\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        \n",
    "        # The loss being optimized is a combination of the reconstruction and regularization losses.\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "        \n",
    "        # To learn the parameters of the autoencoder, an Adam Optimizer can be used used\n",
    "        # Use the Adam Optimizer, with the given learning_rate. To see usage,\n",
    "        # see https://www.tensorflow.org/api_docs/python/tf/train \n",
    "        \n",
    "        # (10 points)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # To perform the training, a training operation is defined. The task of \n",
    "        # the operation is to minimize the loss passed to it. \n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        # Initialize all trainable variables in one go, before training starts. \n",
    "        # To do so use the global_variables_initializer() function of tensorflow.  \n",
    "        # See https://www.tensorflow.org/api_guides/python/state_ops#Variable_helper_functions \n",
    "        \n",
    "        # (10 points)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    # Now the computational graph is defined. We can now pass data, and perform training.\n",
    "   \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        # Call the initializer defined above. \n",
    "        init.run()\n",
    "        \n",
    "        # An epoch comprises of passing all the samples in the training set\n",
    "        # to the algorithm once. \n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            # The training is performed in batch of size batch_size. The number of batches can be computed.\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            \n",
    "            # Go through all the batches..\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                \n",
    "                # To run the training once, pass the training operation and the data.\n",
    "                # In tensorflow, to pass the data to the placeholders, feed_dict is used. \n",
    "               \n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "             \n",
    "            # After going through an entire epoch, we want to know the training loss.\n",
    "            # To do so, we need to evaluate the reconstruction loss node in the graph.\n",
    "            # Pass the appropriate data in the function below to do so. \n",
    "            \n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            \n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "            \n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        \n",
    "        # return the needed parameters\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], \\\n",
    "               params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.0188739\n",
      "1 Train MSE: 0.0179402\n",
      "2 Train MSE: 0.0197976\n",
      "3 Train MSE: 0.0195595\n",
      "4 Train MSE: 0.0191635\n",
      "5 Train MSE: 0.019063\n",
      "6 Train MSE: 0.0194744\n",
      "7 Train MSE: 0.0192214\n",
      "8 Train MSE: 0.0202738\n",
      "9 Train MSE: 0.0200159\n"
     ]
    }
   ],
   "source": [
    "# Now run the training of the autoencoder, using the MNIST training images. \n",
    "# Complete the code below to use 100 hidden neurons, and train for 10 epochs\n",
    "# with a batch size of 150\n",
    "\n",
    "# (5 points)\n",
    "simple_output, W1_simple, b1_simple, W2_simple, b2_simple = train_autoencoder(\n",
    "    mnist.train.images, 100,10,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the weights W1, b1, W2 and b2. \n",
    "\n",
    "Using these, we can run test images through the autoencoder and see the results. We will do that later. Before that, let's build a stacked autoencoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Building a stacked autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train two autoencoders. The first one is trained on the training data, and the second is trained on the previous Autoencoder's hidden layer output. To do so, we can use the train_autoencoder() function we defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code similar to the previous line to train two autoencoders. We did not use the first output above. \n",
    "We will use it here as the intermediate output, which will be used to train the second autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.0183982\n",
      "1 Train MSE: 0.0177385\n",
      "2 Train MSE: 0.0196133\n",
      "3 Train MSE: 0.0193563\n",
      "4 Train MSE: 0.0190451\n",
      "5 Train MSE: 0.0190034\n",
      "6 Train MSE: 0.0194233\n",
      "7 Train MSE: 0.0192226\n",
      "8 Train MSE: 0.0202485\n",
      "9 Train MSE: 0.0198858\n",
      "0 Train MSE: 0.00376496\n",
      "1 Train MSE: 0.00409651\n",
      "2 Train MSE: 0.00424412\n",
      "3 Train MSE: 0.00430497\n",
      "4 Train MSE: 0.00409228\n",
      "5 Train MSE: 0.00431693\n",
      "6 Train MSE: 0.00420121\n",
      "7 Train MSE: 0.00425522\n",
      "8 Train MSE: 0.00417095\n",
      "9 Train MSE: 0.00403692\n"
     ]
    }
   ],
   "source": [
    "# Train a stacked autoencoder using 300 neurons in the first layer, and 150 neurons in the next layer, for \n",
    "# 10 epochs each, with a batchsize of 150.\n",
    "# (15 points)\n",
    "\n",
    "hidden_output, W1_stacked, b1_stacked, W4_stacked, b4_stacked = train_autoencoder(mnist.train.images, 300, 10, 150)\n",
    "stacked_output, W2_stacked, b2_stacked, W3_stacked, b3_stacked = train_autoencoder(hidden_output, 150, 10, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Visualizing the Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualize the reconstructions on test digits. Let's first define a function to show the reconstructed digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_reconstructed_digits(X, outputs, filename='', n_test_digits = 10):\n",
    "    with tf.Session() as sess:\n",
    "        X_test = mnist.test.images[:n_test_digits]\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_test})\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 3 * n_test_digits))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(X_test[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])\n",
    "    if not filename == '':\n",
    "        plt.savefig(filename)\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the reconstruction of the **simple** autoencoder first. Fill in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28 # size of the MNIST images\n",
    "\n",
    "# create a placeholder of type float32, with fixed second dimension = n_inputs. \n",
    "#### YOUR CODE HERE\n",
    "# (5 points)\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs))\n",
    "\n",
    "# create the hidden layer and ouptut layer. Remember that \n",
    "# hidden = activation_function(W1*X + b1) and\n",
    "# choose your activation function to be the exponential linear unit \n",
    "# using tf.nn.elu() and tf.matmul() for matrix multiplication.\n",
    "# (5 points)\n",
    "hidden1 = tf.nn.elu(tf.matmul(X, W1_simple) + b1_simple)\n",
    "\n",
    "# output = (hidden1*W2 + b2) [NO ACTIVATION HERE]\n",
    "# (5 points)\n",
    "outputs = (tf.matmul(hidden1,W2_simple) + b2_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the reconstructed digits as an appropriate png file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_reconstructed_digits(X, outputs, 'simple.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for the **stacked** autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "\n",
    "# Use that\n",
    "# hidden1 = activation_function(W1*X + b1)\n",
    "# hidden2 = activation_function(W2*hidden1 + b2)\n",
    "# hidden3 = activation_function(W3*hidden2 + b3)\n",
    "# output = W4*hidden3 + b4\n",
    "\n",
    "\n",
    "# (15 points)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "hidden1 = tf.nn.elu(tf.matmul(X, W1_stacked) + b1_stacked) \n",
    "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2_stacked) + b2_stacked)\n",
    "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3_stacked) + b3_stacked)\n",
    "outputs = tf.matmul(hidden3, W4_stacked) + b4_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_reconstructed_digits(X, outputs, 'stacked_300_50.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit\n",
    "(10 points)\n",
    "* Build a stacked autoencoder with 3 layers, similar to the above.\n",
    "* Tune the number of hidden neurons to get the best output (visually). \n",
    "* Save the corresponding output images with appropriate names. \n",
    "\n",
    "(5 points)\n",
    "* Do you think adding another hidden layer was helpful? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.0183982\n",
      "1 Train MSE: 0.0177385\n",
      "2 Train MSE: 0.0196133\n",
      "3 Train MSE: 0.0193563\n",
      "4 Train MSE: 0.0190451\n",
      "5 Train MSE: 0.0190034\n",
      "6 Train MSE: 0.0194233\n",
      "7 Train MSE: 0.0192226\n",
      "8 Train MSE: 0.0202485\n",
      "9 Train MSE: 0.0198858\n",
      "0 Train MSE: 0.00376496\n",
      "1 Train MSE: 0.00409651\n",
      "2 Train MSE: 0.00424412\n",
      "3 Train MSE: 0.00430497\n",
      "4 Train MSE: 0.00409228\n",
      "5 Train MSE: 0.00431693\n",
      "6 Train MSE: 0.00420121\n",
      "7 Train MSE: 0.00425522\n",
      "8 Train MSE: 0.00417095\n",
      "9 Train MSE: 0.00403692\n",
      "0 Train MSE: 0.00193519\n",
      "1 Train MSE: 0.00228511\n",
      "2 Train MSE: 0.00218026\n",
      "3 Train MSE: 0.00224638\n",
      "4 Train MSE: 0.00241536\n",
      "5 Train MSE: 0.002187\n",
      "6 Train MSE: 0.00232828\n",
      "7 Train MSE: 0.00230324\n",
      "8 Train MSE: 0.00239218\n",
      "9 Train MSE: 0.00230837\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE FOR EXTRA CREDIT\n",
    "hidden_output1, W1_stacked, b1_stacked, W6_stacked, b6_stacked = train_autoencoder(mnist.train.images, 300, 10, 150)\n",
    "hidden_output2, W2_stacked, b2_stacked, W5_stacked, b5_stacked = train_autoencoder(hidden_output1, 150, 10, 150)\n",
    "hidden_output3, W3_stacked, b3_stacked, W4_stacked, b4_stacked = train_autoencoder(hidden_output2, 150, 10, 150)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "hidden1 = tf.nn.elu(tf.matmul(X, W1_stacked) + b1_stacked)\n",
    "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2_stacked) + b2_stacked)\n",
    "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3_stacked) + b3_stacked)\n",
    "hidden4 = tf.nn.elu(tf.matmul(hidden3, W4_stacked) + b4_stacked)\n",
    "hidden5 = tf.nn.elu(tf.matmul(hidden4, W5_stacked) + b5_stacked)\n",
    "outputs = tf.matmul(hidden5, W6_stacked) + b6_stacked\n",
    "\n",
    "show_reconstructed_digits(X, outputs, '3_layer_300_50.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why is n_outputs = n_inputs in the train_autoencoder() function? (5 points)\n",
    "* Do autoencoders come under supervised learning algorithms, or unsupervised learning algorithms? Explain. (5 points)\n",
    "* Vary the number of neurons in the hidden layer of the 2-layer stacked autoencoder.  (15 points)\n",
    "\n",
    "    Try out the following number of hidden neurons, and save the reconstructions for [hidden_1, hidden_2] = \n",
    "    * [ 25, 10]\n",
    "    * [ 50, 25]\n",
    "    * [100, 35]\n",
    "    * [300, 50]\n",
    "* Explain the variation. (5 points) \n",
    "* Compare the reconstruction of the simple autoencoder vs the stacked autoencoders (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_outputs is equal to n_inputs in the train_autoencoder() function so that the whole picture can be rebuilt.\n",
    "\n",
    "Autoencoders come under unsupervised learning because there is no prior defined labels on the training data.\n",
    "\n",
    "Some of the neuron pairs produce clearer images versus other ones. I could not really establish a correlation between the layers though.\n",
    "\n",
    "The reconstruction of the simple autoencoder looked slightly better than that of the stacked autoencoders in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_neurons(hidden_1, hidden_2, title):\n",
    "    \n",
    "    hidden_output, W1_stacked, b1_stacked, W4_stacked, b4_stacked = train_autoencoder(mnist.train.images, hidden_1, 10, 150)\n",
    "    stacked_output, W2_stacked, b2_stacked, W3_stacked, b3_stacked = train_autoencoder(hidden_output, hidden_2, 10, 150)\n",
    "    \n",
    "    reset_graph()\n",
    "\n",
    "    n_inputs = 28*28\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "    hidden1 = tf.nn.elu(tf.matmul(X, W1_stacked) + b1_stacked) \n",
    "    hidden2 = tf.nn.elu(tf.matmul(hidden1, W2_stacked) + b2_stacked)\n",
    "    hidden3 = tf.nn.elu(tf.matmul(hidden2, W3_stacked) + b3_stacked)\n",
    "    outputs = tf.matmul(hidden3, W4_stacked) + b4_stacked\n",
    "    \n",
    "    show_reconstructed_digits(X, outputs, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.0277956\n",
      "1 Train MSE: 0.0223349\n",
      "2 Train MSE: 0.0239603\n",
      "3 Train MSE: 0.0235846\n",
      "4 Train MSE: 0.0228943\n",
      "5 Train MSE: 0.0231197\n",
      "6 Train MSE: 0.0233736\n",
      "7 Train MSE: 0.0226926\n",
      "8 Train MSE: 0.0239273\n",
      "9 Train MSE: 0.0234951\n",
      "0 Train MSE: 0.334457\n",
      "1 Train MSE: 0.265359\n",
      "2 Train MSE: 0.29439\n",
      "3 Train MSE: 0.283808\n",
      "4 Train MSE: 0.281086\n",
      "5 Train MSE: 0.278161\n",
      "6 Train MSE: 0.260879\n",
      "7 Train MSE: 0.268513\n",
      "8 Train MSE: 0.272943\n",
      "9 Train MSE: 0.263226\n",
      "0 Train MSE: 0.0206827\n",
      "1 Train MSE: 0.0182166\n",
      "2 Train MSE: 0.0197283\n",
      "3 Train MSE: 0.0195614\n",
      "4 Train MSE: 0.0190983\n",
      "5 Train MSE: 0.0189817\n",
      "6 Train MSE: 0.0192776\n",
      "7 Train MSE: 0.0190889\n",
      "8 Train MSE: 0.0200199\n",
      "9 Train MSE: 0.0199175\n",
      "0 Train MSE: 0.0413285\n",
      "1 Train MSE: 0.0371483\n",
      "2 Train MSE: 0.036482\n",
      "3 Train MSE: 0.0369612\n",
      "4 Train MSE: 0.0337837\n",
      "5 Train MSE: 0.0356899\n",
      "6 Train MSE: 0.0355617\n",
      "79% Train MSE: 0.039513\n",
      "8 Train MSE: 0.0380454\n",
      "9 Train MSE: 0.033611\n",
      "0 Train MSE: 0.0188739\n",
      "1 Train MSE: 0.0179402\n",
      "2 Train MSE: 0.0197976\n",
      "3 Train MSE: 0.0195595\n",
      "4 Train MSE: 0.0191635\n",
      "5 Train MSE: 0.019063\n",
      "6 Train MSE: 0.0194744\n",
      "7 Train MSE: 0.0192214\n",
      "8 Train MSE: 0.0202738\n",
      "9 Train MSE: 0.0200159\n",
      "0 Train MSE: 0.00710891\n",
      "1 Train MSE: 0.00447647\n",
      "2 Train MSE: 0.00449859\n",
      "3 Train MSE: 0.00439892\n",
      "49% Train MSE: 0.00436108\n",
      "5 Train MSE: 0.00428817\n",
      "6 Train MSE: 0.00424189\n",
      "7 Train MSE: 0.00439998\n",
      "8 Train MSE: 0.00439062\n",
      "9 Train MSE: 0.00405372\n",
      "0 Train MSE: 0.0183982\n",
      "1 Train MSE: 0.0177385\n",
      "2 Train MSE: 0.0196133\n",
      "3 Train MSE: 0.0193563\n",
      "4 Train MSE: 0.0190451\n",
      "5 Train MSE: 0.0190034\n",
      "6 Train MSE: 0.0194233\n",
      "7 Train MSE: 0.0192226\n",
      "8 Train MSE: 0.0202485\n",
      "9 Train MSE: 0.0198858\n",
      "0 Train MSE: 0.00394411\n",
      "1 Train MSE: 0.00401347\n",
      "2 Train MSE: 0.00436969\n",
      "3 Train MSE: 0.00418254\n",
      "4 Train MSE: 0.00384441\n",
      "5 Train MSE: 0.00405969\n",
      "6 Train MSE: 0.0039575\n",
      "7 Train MSE: 0.00421822\n",
      "8 Train MSE: 0.00408941\n",
      "9 Train MSE: 0.00399171\n"
     ]
    }
   ],
   "source": [
    "test_neurons(25, 10, \"25_10_stacked.png\")\n",
    "test_neurons(50,25, \"50_25_stacked.png\")\n",
    "test_neurons(100,35, \"100_35_stacked.png\")\n",
    "test_neurons(300, 50, \"300_50_stacked.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {
   "height": "381px",
   "width": "453px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
