{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 - Latent Dirichlet Allocation (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(30 points)\n",
    "\n",
    "In this part of the lab, you will use the Latent Dirichlet Allocation approach to analyze the latent topics of two different datasets. \n",
    "\n",
    "Beside this iPython notebook, you are also given:\n",
    "\n",
    "1. ingredients.xls: Each row represents one Medieval European recipe (in terms of ingredient list). The first cell of each row denotes the number of ingredients in this row, but there may be errors (you should write code to handle such cases). \n",
    "\n",
    "### What to submit\n",
    "* Completed IPYNB with answers to questions at the end\n",
    "* For instructions on submission, please go to webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Latent Dirichlet Allocation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to implement anything in this part, but should read it to understand it.  You will use it when analysing the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, re, time, string\n",
    "import numpy as n\n",
    "from scipy.special import gammaln, psi\n",
    "\n",
    "n.random.seed(100000001)\n",
    "meanchangethresh = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dirichlet_expectation(alpha):\n",
    "    if (len(alpha.shape) == 1):\n",
    "        return(psi(alpha) - psi(n.sum(alpha)))\n",
    "    return(psi(alpha) - psi(n.sum(alpha, 1))[:, n.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_doc_list(docs, vocab):\n",
    "    if (type(docs).__name__ == 'str'):\n",
    "        temp = list()\n",
    "        temp.append(docs)\n",
    "        docs = temp\n",
    "\n",
    "    D = len(docs)\n",
    "    \n",
    "    wordids = list()\n",
    "    wordcts = list()\n",
    "    for d in range(0, D):\n",
    "        docs[d] = docs[d].lower()\n",
    "        docs[d] = re.sub(r'-', ' ', docs[d])\n",
    "        docs[d] = re.sub(r'[^a-z ]', '', docs[d])\n",
    "        docs[d] = re.sub(r' +', ' ', docs[d])\n",
    "        words = string.split(docs[d])\n",
    "        ddict = dict()\n",
    "        for word in words:\n",
    "            if (word in vocab):\n",
    "                wordtoken = vocab[word]\n",
    "                if (not wordtoken in ddict):\n",
    "                    ddict[wordtoken] = 0\n",
    "                ddict[wordtoken] += 1\n",
    "        wordids.append(ddict.keys())\n",
    "        wordcts.append(ddict.values())\n",
    "\n",
    "    return((wordids, wordcts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OnlineLDA:\n",
    "    \"\"\"\n",
    "    Implements online Variational Bayes for LDA as described in (Hoffman et al. 2010).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, K, D, alpha, eta, tau0, kappa):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        K: Number of topics\n",
    "        vocab: A set of words to recognize. When analyzing documents, any word\n",
    "           not in this set will be ignored.\n",
    "        D: Total number of documents in the population. For a fixed corpus,\n",
    "           this is the size of the corpus. In the truly online setting, this\n",
    "           can be an estimate of the maximum number of documents that\n",
    "           could ever be seen.\n",
    "        alpha: Hyperparameter for prior on weight vectors theta\n",
    "        eta: Hyperparameter for prior on topics beta\n",
    "        tau0: A (positive) learning parameter that downweights early iterations\n",
    "        kappa: Learning rate: exponential decay rate---should be between\n",
    "             (0.5, 1.0] to guarantee asymptotic convergence.\n",
    "\n",
    "        Note that if you pass the same set of D documents in every time and\n",
    "        set kappa=0 this class can also be used to do batch VB.\n",
    "        \"\"\"\n",
    "        self._vocab = dict()\n",
    "        for word in vocab:\n",
    "            #word = word.lower()\n",
    "            #word = re.sub(r'[^a-z]', '', word)\n",
    "            self._vocab[word] = len(self._vocab)\n",
    "\n",
    "        self._K = K\n",
    "        self._W = len(self._vocab)\n",
    "        self._D = D\n",
    "        self._alpha = alpha\n",
    "        self._eta = eta\n",
    "        self._tau0 = tau0 + 1\n",
    "        self._kappa = kappa\n",
    "        self._updatect = 0\n",
    "\n",
    "        # Initialize the variational distribution q(beta|lambda)\n",
    "        self._lambda = 1*n.random.gamma(100., 1./100., (self._K, self._W))\n",
    "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
    "        self._expElogbeta = n.exp(self._Elogbeta)\n",
    "\n",
    "    def do_e_step(self, docs):\n",
    "        \"\"\"\n",
    "        Given a mini-batch of documents, estimates the parameters\n",
    "        gamma controlling the variational distribution over the topic\n",
    "        weights for each document in the mini-batch.\n",
    "\n",
    "        Arguments:\n",
    "        docs:  List of D documents. Each document must be represented\n",
    "               as a string. (Word order is unimportant.) Any\n",
    "               words not in the vocabulary will be ignored.\n",
    "\n",
    "        Returns a tuple containing the estimated values of gamma,\n",
    "        as well as sufficient statistics needed to update lambda.\n",
    "        \"\"\"\n",
    "        # This is to handle the case where someone just hands us a single\n",
    "        # document, not in a list.\n",
    "        if (type(docs).__name__ == 'string'):\n",
    "            temp = list()\n",
    "            temp.append(docs)\n",
    "            docs = temp\n",
    "\n",
    "        (wordids, wordcts) = parse_doc_list(docs, self._vocab)\n",
    "        batchD = len(docs)\n",
    "\n",
    "        # Initialize the variational distribution q(theta|gamma) for\n",
    "        # the mini-batch\n",
    "        gamma = 1*n.random.gamma(100., 1./100., (batchD, self._K))\n",
    "        Elogtheta = dirichlet_expectation(gamma)\n",
    "        expElogtheta = n.exp(Elogtheta)\n",
    "\n",
    "        sstats = n.zeros(self._lambda.shape)\n",
    "        # Now, for each document d update that document's gamma and phi\n",
    "        it = 0\n",
    "        meanchange = 0\n",
    "        for d in range(0, batchD):\n",
    "            # These are mostly just shorthand (but might help cache locality)\n",
    "            ids = wordids[d]\n",
    "            cts = wordcts[d]\n",
    "            gammad = gamma[d, :]\n",
    "            Elogthetad = Elogtheta[d, :]\n",
    "            expElogthetad = expElogtheta[d, :]\n",
    "            expElogbetad = self._expElogbeta[:, ids]\n",
    "            # The optimal phi_{dwk} is proportional to \n",
    "            # expElogthetad_k * expElogbetad_w. phinorm is the normalizer.\n",
    "            phinorm = n.dot(expElogthetad, expElogbetad) + 1e-100\n",
    "            # Iterate between gamma and phi until convergence\n",
    "            for it in range(0, 100):\n",
    "                lastgamma = gammad\n",
    "                # We represent phi implicitly to save memory and time.\n",
    "                # Substituting the value of the optimal phi back into\n",
    "                # the update for gamma gives this update. Cf. Lee&Seung 2001.\n",
    "                gammad = self._alpha + expElogthetad * \\\n",
    "                    n.dot(cts / phinorm, expElogbetad.T)\n",
    "                Elogthetad = dirichlet_expectation(gammad)\n",
    "                expElogthetad = n.exp(Elogthetad)\n",
    "                phinorm = n.dot(expElogthetad, expElogbetad) + 1e-100\n",
    "                # If gamma hasn't changed much, we're done.\n",
    "                meanchange = n.mean(abs(gammad - lastgamma))\n",
    "                if (meanchange < meanchangethresh):\n",
    "                    break\n",
    "            gamma[d, :] = gammad\n",
    "            # Contribution of document d to the expected sufficient\n",
    "            # statistics for the M step.\n",
    "            sstats[:, ids] += n.outer(expElogthetad.T, cts/phinorm)\n",
    "\n",
    "        # This step finishes computing the sufficient statistics for the\n",
    "        # M step, so that\n",
    "        # sstats[k, w] = \\sum_d n_{dw} * phi_{dwk} \n",
    "        # = \\sum_d n_{dw} * exp{Elogtheta_{dk} + Elogbeta_{kw}} / phinorm_{dw}.\n",
    "        sstats = sstats * self._expElogbeta\n",
    "\n",
    "        return((gamma, sstats))\n",
    "\n",
    "    def update_lambda(self, docs):\n",
    "        \"\"\"\n",
    "        First does an E step on the mini-batch given in wordids and\n",
    "        wordcts, then uses the result of that E step to update the\n",
    "        variational parameter matrix lambda.\n",
    "\n",
    "        Arguments:\n",
    "        docs:  List of D documents. Each document must be represented\n",
    "               as a string. (Word order is unimportant.) Any\n",
    "               words not in the vocabulary will be ignored.\n",
    "\n",
    "        Returns gamma, the parameters to the variational distribution\n",
    "        over the topic weights theta for the documents analyzed in this\n",
    "        update.\n",
    "\n",
    "        Also returns an estimate of the variational bound for the\n",
    "        entire corpus for the OLD setting of lambda based on the\n",
    "        documents passed in. This can be used as a (possibly very\n",
    "        noisy) estimate of held-out likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # rhot will be between 0 and 1, and says how much to weight\n",
    "        # the information we got from this mini-batch.\n",
    "        rhot = pow(self._tau0 + self._updatect, -self._kappa)\n",
    "        self._rhot = rhot\n",
    "        # Do an E step to update gamma, phi | lambda for this\n",
    "        # mini-batch. This also returns the information about phi that\n",
    "        # we need to update lambda.\n",
    "        (gamma, sstats) = self.do_e_step(docs)\n",
    "        # Estimate held-out likelihood for current values of lambda.\n",
    "        bound = self.approx_bound(docs, gamma)\n",
    "        # Update lambda based on documents.\n",
    "        self._lambda = self._lambda * (1-rhot) + \\\n",
    "            rhot * (self._eta + self._D * sstats / len(docs))\n",
    "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
    "        self._expElogbeta = n.exp(self._Elogbeta)\n",
    "        self._updatect += 1\n",
    "\n",
    "        return(gamma, bound)\n",
    "\n",
    "    def approx_bound(self, docs, gamma):\n",
    "        \"\"\"\n",
    "        Estimates the variational bound over *all documents* using only\n",
    "        the documents passed in as \"docs.\" gamma is the set of parameters\n",
    "        to the variational distribution q(theta) corresponding to the\n",
    "        set of documents passed in.\n",
    "\n",
    "        The output of this function is going to be noisy, but can be\n",
    "        useful for assessing convergence.\n",
    "        \"\"\"\n",
    "\n",
    "        # This is to handle the case where someone just hands us a single\n",
    "        # document, not in a list.\n",
    "        if (type(docs).__name__ == 'string'):\n",
    "            temp = list()\n",
    "            temp.append(docs)\n",
    "            docs = temp\n",
    "\n",
    "        (wordids, wordcts) = parse_doc_list(docs, self._vocab)\n",
    "        batchD = len(docs)\n",
    "\n",
    "        score = 0\n",
    "        Elogtheta = dirichlet_expectation(gamma)\n",
    "        expElogtheta = n.exp(Elogtheta)\n",
    "\n",
    "        # E[log p(docs | theta, beta)]\n",
    "        for d in range(0, batchD):\n",
    "            gammad = gamma[d, :]\n",
    "            ids = wordids[d]\n",
    "            cts = n.array(wordcts[d])\n",
    "            phinorm = n.zeros(len(ids))\n",
    "            for i in range(0, len(ids)):\n",
    "                temp = Elogtheta[d, :] + self._Elogbeta[:, ids[i]]\n",
    "                tmax = max(temp)\n",
    "                phinorm[i] = n.log(sum(n.exp(temp - tmax))) + tmax\n",
    "            score += n.sum(cts * phinorm)\n",
    "#             oldphinorm = phinorm\n",
    "#             phinorm = n.dot(expElogtheta[d, :], self._expElogbeta[:, ids])\n",
    "#             print oldphinorm\n",
    "#             print n.log(phinorm)\n",
    "#             score += n.sum(cts * n.log(phinorm))\n",
    "\n",
    "        # E[log p(theta | alpha) - log q(theta | gamma)]\n",
    "        score += n.sum((self._alpha - gamma)*Elogtheta)\n",
    "        score += n.sum(gammaln(gamma) - gammaln(self._alpha))\n",
    "        score += sum(gammaln(self._alpha*self._K) - gammaln(n.sum(gamma, 1)))\n",
    "\n",
    "        # Compensate for the subsampling of the population of documents\n",
    "        score = score * self._D / len(docs)\n",
    "\n",
    "        # E[log p(beta | eta) - log q (beta | lambda)]\n",
    "        score = score + n.sum((self._eta-self._lambda)*self._Elogbeta)\n",
    "        score = score + n.sum(gammaln(self._lambda) - gammaln(self._eta))\n",
    "        score = score + n.sum(gammaln(self._eta*self._W) - \n",
    "                              gammaln(n.sum(self._lambda, 1)))\n",
    "\n",
    "        return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: The ingredient dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle, string, numpy, getopt, sys, random, time, re, pprint\n",
    "from xlrd import open_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Collect the vocabulary and the document lists \n",
    "\n",
    "    vocab: a list contains all the unique terms; (e.g. vocab[0] = \"bean\", vocab[1] = \"broth\")\n",
    "\n",
    "    docset: a list contains all the receipts (e.g. docset[1] = \"bean broth bacon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wb = open_workbook('ingredients.xls')\n",
    "ws = wb.sheet_by_name('ingredients')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "docset = []\n",
    "num_vocab = 0\n",
    "\n",
    "#### 1. WRITE YOUR CODE HERE #### \n",
    "# (10 points)\n",
    "# should be similar to the first code in Part 1 of this lab, \n",
    "# but NOT exactly the same. vocab is a list here, containing unique terms.\n",
    "temp_list = []\n",
    "for i in range(ws.nrows):\n",
    "    num_words = ws.cell_value(i,0)\n",
    "    for j in range(1,int(num_words)+1):\n",
    "        if ws.cell_value(i,j) == '' or ws.cell_value(i,j) == None:\n",
    "            continue\n",
    "        if ws.cell_value(i,j) not in vocab:\n",
    "            vocab.append(ws.cell_value(i,j))\n",
    "            num_vocab += 1\n",
    "        temp_list.append(ws.cell_value(i,j))\n",
    "    result = ''\n",
    "    for word in temp_list:\n",
    "        result += word+' '\n",
    "    docset.append(result)\n",
    "    temp_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frumenty wheat oil broth milk almond saffron yolk egg  386\n"
     ]
    }
   ],
   "source": [
    "print docset[0], len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Decide the parameters needed\n",
    "Decide the number of topics as you wish to recover from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide the number of documents to analyze at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = len(vocab)\n",
    "D = len(docset)\n",
    "documentstoanalyze = int(D/batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the algorithm with alpha=1/K, eta=1/K, tau_0=1024, kappa=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Fill in function arguments for the Online LDA algorithm\n",
    "# (5 points)\n",
    "olda_ingredient = OnlineLDA(vocab, K, D, 1.0/K, 1.0/K, 1024, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide the total number of data passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pass = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform the online training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided this code. You should run it and interpret the results later. \n",
    "\n",
    "The held-out perplexity is the most common way to evaluate a probabilistic model, by measuring the log-likelihood of a held-out test set.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: Held-Out perplexity estimate\n",
      "0:  1037.565911\n",
      "1:  488.511976\n",
      "2:  326.263653\n",
      "3:  233.976321\n",
      "4:  209.422146\n",
      "5:  200.620250\n",
      "6:  217.801180\n",
      "7:  195.063699\n",
      "8:  170.065494\n",
      "9:  170.081268\n",
      "10:  159.271088\n",
      "11:  175.687720\n",
      "12:  150.514830\n",
      "13:  168.813606\n",
      "14:  175.857747\n",
      "15:  137.635929\n",
      "16:  140.310316\n",
      "17:  140.864815\n",
      "18:  138.339653\n",
      "19:  136.408191\n",
      "20:  148.278282\n",
      "21:  143.975898\n",
      "22:  149.195784\n",
      "23:  134.434910\n",
      "24:  129.253549\n",
      "25:  149.378892\n",
      "26:  146.627100\n",
      "27:  135.688642\n",
      "28:  144.686519\n",
      "29:  136.679787\n",
      "30:  135.788388\n",
      "31:  146.845143\n",
      "32:  147.580788\n",
      "33:  142.260414\n",
      "34:  151.775968\n",
      "35:  151.809844\n",
      "36:  138.182309\n",
      "37:  136.654602\n",
      "38:  151.031837\n",
      "39:  119.572652\n",
      "40:  131.974619\n",
      "41:  134.480431\n",
      "42:  139.358076\n",
      "43:  136.778725\n",
      "44:  142.704234\n",
      "45:  131.198295\n",
      "46:  140.338717\n",
      "47:  140.608894\n",
      "48:  134.865832\n",
      "49:  136.335010\n",
      "50:  137.429220\n",
      "51:  129.816195\n",
      "52:  126.463670\n",
      "53:  127.670785\n",
      "54:  123.768791\n",
      "55:  144.553437\n",
      "56:  159.510529\n",
      "57:  144.965538\n",
      "58:  143.357466\n",
      "59:  120.755181\n",
      "60:  134.548972\n",
      "61:  135.250341\n",
      "62:  138.153548\n",
      "63:  142.352771\n"
     ]
    }
   ],
   "source": [
    "print 'Iteration: Held-Out perplexity estimate'\n",
    "for iteration in range(0, n_pass*documentstoanalyze):\n",
    "    \n",
    "    perm_indexes = numpy.random.permutation(range(D))[0:batchsize]\n",
    "    #perm_indexes = range(iteration*batchsize, (iteration+1)*batchsize)\n",
    "    sub_docset = []\n",
    "    \n",
    "    for idx in perm_indexes:\n",
    "        sub_docset.append(docset[idx])\n",
    "    \n",
    "    # Give them to online LDA\n",
    "    (gamma, bound) = olda_ingredient.update_lambda(sub_docset)\n",
    "    \n",
    "    # Compute an estimate of held-out perplexity\n",
    "    (wordids, wordcts) = parse_doc_list(sub_docset, olda_ingredient._vocab)\n",
    "    perwordbound = bound * len(sub_docset) / (D * sum(map(sum, wordcts)))\n",
    "   \n",
    "    print '%d:  %f' % \\\n",
    "        (iteration, numpy.exp(-perwordbound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Print the top K topics. \n",
    "Use the code below to print out top words in the K topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0:\n",
      "                salt  \t---\t  0.0509\n",
      "              pepper  \t---\t  0.0466\n",
      "                wine  \t---\t  0.0455\n",
      "              ginger  \t---\t  0.0382\n",
      "             vinegar  \t---\t  0.0376\n",
      "               bread  \t---\t  0.0366\n",
      "               broth  \t---\t  0.0341\n",
      "             saffron  \t---\t  0.0297\n",
      "               clove  \t---\t  0.0274\n",
      "            cinnamon  \t---\t  0.0249\n",
      "topic 1:\n",
      "               crane  \t---\t  0.0303\n",
      "               heron  \t---\t  0.0290\n",
      "             peacock  \t---\t  0.0173\n",
      "                plum  \t---\t  0.0145\n",
      "                swan  \t---\t  0.0131\n",
      "                bird  \t---\t  0.0114\n",
      "            pheasant  \t---\t  0.0109\n",
      "              cheese  \t---\t  0.0099\n",
      "             bittern  \t---\t  0.0098\n",
      "              garlic  \t---\t  0.0098\n",
      "topic 2:\n",
      "                bean  \t---\t  0.0326\n",
      "                 oil  \t---\t  0.0194\n",
      "              turnip  \t---\t  0.0188\n",
      "                leek  \t---\t  0.0177\n",
      "               onion  \t---\t  0.0170\n",
      "                 pea  \t---\t  0.0166\n",
      "              fennel  \t---\t  0.0163\n",
      "                beet  \t---\t  0.0150\n",
      "              borage  \t---\t  0.0129\n",
      "               olive  \t---\t  0.0128\n",
      "topic 3:\n",
      "               sugar  \t---\t  0.0858\n",
      "                 egg  \t---\t  0.0608\n",
      "              almond  \t---\t  0.0518\n",
      "                milk  \t---\t  0.0489\n",
      "             saffron  \t---\t  0.0340\n",
      "                yolk  \t---\t  0.0340\n",
      "              ginger  \t---\t  0.0320\n",
      "            cinnamon  \t---\t  0.0303\n",
      "                rice  \t---\t  0.0264\n",
      "                salt  \t---\t  0.0259\n",
      "topic 4:\n",
      "              flower  \t---\t  0.0157\n",
      "                 bay  \t---\t  0.0152\n",
      "            scallion  \t---\t  0.0127\n",
      "                 oil  \t---\t  0.0123\n",
      "              plaice  \t---\t  0.0120\n",
      "             cabbage  \t---\t  0.0117\n",
      "            lavender  \t---\t  0.0113\n",
      "               roach  \t---\t  0.0101\n",
      "                pear  \t---\t  0.0094\n",
      "               liver  \t---\t  0.0089\n"
     ]
    }
   ],
   "source": [
    "num_elements = 10\n",
    "\n",
    "testlambda = olda_ingredient._lambda\n",
    "for k in range(0, K):\n",
    "    lambdak = list(testlambda[k, :])\n",
    "    lambdak = lambdak / sum(lambdak)\n",
    "    temp = zip(lambdak, range(0, len(lambdak)))\n",
    "    temp = sorted(temp, key = lambda x: x[0], reverse=True)\n",
    "    print 'topic %d:' % (k)\n",
    "    for i in range(0, num_elements):\n",
    "        print '%20s  \\t---\\t  %.4f' % (vocab[temp[i][1]], temp[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "### (5+3+3+4 points)\n",
    "   * Do you see any meaning in the top 5 topics you got? Can you identify the topics they are representing (for at least 2 of the 5)?\n",
    "   * What do the numbers against every element signify?\n",
    "   * What estimation method is used to determine the topics and the word distributions?\n",
    "   * What are the alpha and eta parameters in the LDA algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answers here\n",
    "Each of the top five topics look like they represent a certain category within ingredients. For example, topic 1 looks to be of birds, and topic 0 looks like spices and condiments.\n",
    "\n",
    "The numbers against every element looks like its the probability that the certain item appears within the certain topic.\n",
    "\n",
    "The estimation method that is used to determine the topics and the word distributions is the Variational Bayes model.\n",
    "\n",
    "Alpha and eta are parameters that affect the sparsity of theta and lambda distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
