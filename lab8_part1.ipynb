{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 - Latent Semantic Analysis (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(70 points)\n",
    "\n",
    "In the first part of this lab, you will use the Latent Semantic Analysis technique to analyze the latent semantics of a Medieval European recipe corpus. \n",
    "Besides this iPython notebook, you are also given one dataset (ingredients.xls) as follows:\n",
    "\n",
    "Each row represents one recipe (in terms of ingredient list).  The first cell of each row denotes the number of ingredients in this row.\n",
    "\n",
    "You will use this file through the lab, as explained in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What to hand in: \n",
    "You will need to pack following things into a file.\n",
    "\n",
    "\n",
    "   * The completed Notebook files (ipynb) - Remember to answer all the questions in the notebooks!\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import numpy as np\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'ingredients.xls'\n",
    "sheet_name = 'ingredients'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Create a vocabulary dictionary, a term list to save all terms and a document list to save all recipes (in the form of a list of all terms):\n",
    "\n",
    "1. first read the first cell (cell_idx=0) of each row as num_words;\n",
    "2. then read from the first term (cell_idx=1) to the last term (cell_idx = num_words);\n",
    "3. continue if the read term is empty or term == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386 386 386\n"
     ]
    }
   ],
   "source": [
    "wb = xlrd.open_workbook(filename)\n",
    "ws = wb.sheet_by_name(sheet_name)\n",
    "vocab = dict()\n",
    "doc_list = []\n",
    "term_list = []\n",
    "num_vocab = 0   \n",
    "\n",
    "#### 1. COMPLETE THE CODE BELOW ####\n",
    "# (15 points)\n",
    "# Build a vocabulary dictionary (vocab), where the word is the key, and the value is the index of \n",
    "# entry in the dictionary.\n",
    "#\n",
    "# Also, build a document list (doc_list) which saves lists of words in each recipe.\n",
    "# Hint: Use ws.cell_value(i,j) to get the value of the cell at location i,j.\n",
    "# You can use ws.nrows to get number of rows in an open excel sheet\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "for i in range(ws.nrows):\n",
    "    num_words = ws.cell_value(i,0)\n",
    "    for j in range(1,int(num_words)+1):\n",
    "        if ws.cell_value(i,j) == '' or ws.cell_value(i,j) == None:\n",
    "            continue\n",
    "        if ws.cell_value(i,j) not in vocab.keys():\n",
    "            vocab[ws.cell_value(i,j)] = num_vocab\n",
    "            num_vocab += 1\n",
    "        temp_list.append(ws.cell_value(i,j))\n",
    "    doc_list.append(temp_list)\n",
    "    temp_list = []\n",
    "    \n",
    "for keys in vocab.keys():\n",
    "    term_list.append(keys)\n",
    "\n",
    "\n",
    "print len(term_list), num_vocab, len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the term-document occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term-document matrix ''tdMatrix\" describes the occurrences of terms in documents.\n",
    "\n",
    "Each row represents one term, and each column represents one document.\n",
    "\n",
    "tdMatrix[i, j] = 1.0 if the term i occurs in the document j.\n",
    "else tdMatrix[i, j] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_docs = len(doc_list)\n",
    "tdMatrix = np.zeros((num_vocab, num_docs), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### 2. WRITE YOUR CODE HERE TO FILL IN THE TERM DOCUMENT OCCURRENCE MATRIX ####\n",
    "# Use the vocabulary dictionary effectively.\n",
    "# (10 points)\n",
    "for j in range(num_docs):\n",
    "    for word in doc_list[j]:\n",
    "        if word in vocab.keys():\n",
    "            tdMatrix[vocab[word]][j] = 1.0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: SVD decomposition of the term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the numpy function ''np.linalg.svd()\" to do the SVD decomposition. Remember to set the parameter ''full_matrices\" to False.\n",
    "\n",
    "Please check http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html for more details.\n",
    "\n",
    "tdMatrix = U \\* diagonal (s) \\* V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 3. YOUR CODE HERE ####\n",
    "# (5 points)\n",
    "U, s, V = np.linalg.svd(tdMatrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386, 386) (386, 4133) (386,) (386, 386)\n"
     ]
    }
   ],
   "source": [
    "S = np.diag(s)\n",
    "print U.shape, V.shape, s.shape, S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the feature dimension of the latent semantic matrices.\n",
    "\n",
    "The largest possible feature dimension should be smaller than min(tdMatrix.shape[0], tdMatrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_ftr_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the latent semantic matrices for terms and documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the first ''lower_ftr_dim\" columns of U and the first ''lower_ftr_dim\" columns of **transpose of V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(386, 300) (4133, 300) (386, 386)\n"
     ]
    }
   ],
   "source": [
    "#### 4. YOUR CODE HERE ####\n",
    "# (5 points)\n",
    "U = U[:,:lower_ftr_dim]\n",
    "s = S\n",
    "V_transpose = np.transpose(V)[:,:lower_ftr_dim]\n",
    "print U.shape, V_transpose.shape, s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Function to return the indices of the most similar terms/documents to a selected term/document by using the cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has the following inputs:\n",
    "    \n",
    "    1. id: the index of the selected term/document;\n",
    "    \n",
    "    2. ftr_mtx: the latent semantic feature matrix;\n",
    "        \n",
    "    3. top_k: the number of most similar terms/documents to be returned;\n",
    "\n",
    "The function returns a list which contains the indexes of the most similar terms/documents. Note that this function can be used for both terms and documents, by providing the appropriate feature matrix.\n",
    "\n",
    "The cosine similarity(A, B) = (the dot product of A and B)/(norm(A) * norm(B)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### 5. COMPLETE THE FUNCTION BELOW ####\n",
    "# (25 points)\n",
    "    \n",
    "def MostSimilar(term_id, ftr_mtx, top_k):\n",
    "    \n",
    "    target = ftr_mtx[term_id] # the target term/document you want to find terms/documents similar to\n",
    "    \n",
    "    # a) First compute the cosine similarity matrix between every element in ftr_mtx and target\n",
    "    # add 'eps' value to the denominator of the cosine similarity you calculate, for numerical stability\n",
    "    \n",
    "    sim_list = [] # similarity list: List of similarity values of target and every element in ftr_mtx\n",
    "    for i in range(len(ftr_mtx)):\n",
    "        cos_sim = np.dot(target, ftr_mtx[i])/((np.linalg.norm(target)*np.linalg.norm(ftr_mtx[i]))+eps)\n",
    "        sim_list.append(cos_sim)\n",
    "    \n",
    "    \n",
    "    # b) Will the maximum similarity always be 1? If yes, write code to correct for it. \n",
    "    index_arr = np.argsort(sim_list) \n",
    "    ret_arr = index_arr[::-1]\n",
    "    ret_list = ret_arr[1:top_k+1] # list to be returned\n",
    "    # c) Iteratively find the top_k indices based on the similarity matrix and return\n",
    "    # the argument of the entry in the ret_list\n",
    "    \n",
    "    # for j in range(top_k):\n",
    "        \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6a: Find the most similar top_k terms for a chosen term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the top k entries related to a chosen_term of your choice\n",
    "top_k = 5\n",
    "chosen_term = 'apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected term is apple\n",
      "the top 5 similar terms are \n",
      "[u'skylark', u'parsnip', u'peach', u'veal', u'patience']\n"
     ]
    }
   ],
   "source": [
    "if chosen_term in term_list:\n",
    "    print 'the selected term is ' + chosen_term\n",
    "    count = 0\n",
    "    for term in term_list:\n",
    "        if term==chosen_term:\n",
    "            break\n",
    "        count += 1\n",
    "    indexes = MostSimilar(count, U, top_k)\n",
    "    similar_terms = []\n",
    "    print 'the top ' + str(top_k) + ' similar terms are '\n",
    "    for id in indexes:\n",
    "        similar_terms.append(term_list[id])\n",
    "    print similar_terms\n",
    "else:\n",
    "    print 'the selected term is not in the vocabulary!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6b: Find the most similar top_k documents for a chosen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4133\n"
     ]
    }
   ],
   "source": [
    "# Find the top k documents for a document (id) of your choice\n",
    "top_k = 3\n",
    "chosen_doc_id = 100\n",
    "print len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the selected doc is [u'mussel', u'wine', u'broth', u'almond', u'milk', u'leek', u'pepper', u'clove', u'onion', u'oil', u'saffron', u'verjuice', u'vinegar', u'ginger', u'cinnamon']\n",
      "the top 3 similar docss are \n",
      "[u'mussel', u'wine', u'almond', u'broth', u'verjuice', u'vinegar', u'leek', u'oil', u'onion', u'saffron', u'salt']\n",
      "[u'broth', u'egg', u'oil', u'almond', u'onion', u'ginger', u'cinnamon', u'clove', u'saffron', u'verjuice']\n",
      "[u'kidney', u'bean', u'broth', u'onion', u'oil', u'pepper', u'cinnamon', u'saffron']\n"
     ]
    }
   ],
   "source": [
    "if chosen_doc_id < len(doc_list):\n",
    "    print 'the selected doc is ' + str(doc_list[chosen_doc_id]) \n",
    "    indexes = MostSimilar(chosen_doc_id, V_transpose, top_k)\n",
    "    print 'the top ' + str(top_k) + ' similar docss are '        \n",
    "    for id in indexes:\n",
    "        print doc_list[id]            \n",
    "else:\n",
    "    print 'the selected docs id is out of range!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questions: \n",
    "### (3+3+4 points)\n",
    "Based on your experiments, answer:\n",
    "   * What was the word you chose, and what were the top 5 words similar to it? \n",
    "   * Are you satisfied with the similar words you got for the word you chose? Explain. \n",
    "   \n",
    "   \n",
    "   * What was the document you chose, and what were the top 3 documents similar to it? \n",
    "   * Are you satisfied with the similar documents you got for the word you chose? Explain. \n",
    "   \n",
    "   \n",
    "   * Do you think changing the lower_feature_dimension will affect the results?\n",
    "   * Experiment and explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers Here\n",
    "The word I chose was apple, and the top five words I got similar to it were berry, trencher, skylark, bustard, and mace. Besides berry, I am not really satisfied with the four other words since none of them are fruits. I expected more fruits in the result.\n",
    "\n",
    "The document I chose was [u'mussel', u'wine', u'broth', u'almond', u'milk', u'leek', u'pepper', u'clove', u'onion', u'oil', u'saffron', u'verjuice', u'vinegar', u'ginger', u'cinnamon'] and the top three documents similar to it are \n",
    "\n",
    "[u'mussel', u'wine', u'almond', u'broth', u'verjuice', u'vinegar', u'leek', u'oil', u'onion', u'saffron', u'salt']\n",
    "\n",
    "[u'broth', u'egg', u'oil', u'almond', u'onion', u'ginger', u'cinnamon', u'clove', u'saffron', u'verjuice']\n",
    "\n",
    "[u'kidney', u'bean', u'broth', u'onion', u'oil', u'pepper', u'cinnamon', u'saffron']\n",
    "\n",
    "If we increase the lower_feature_dimension, our results do change, but I am not sure that it makes it a lot more clear. For example, when I set lower_feature_dimension to 300, there was parsnip and peach in the results, which seem a lot closer to apple then other results. It seems to make sense, since there are more entries to get cosine similarities of.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
